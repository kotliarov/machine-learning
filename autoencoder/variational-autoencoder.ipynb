{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tv.transforms.Compose([tv.transforms.ToTensor()])\n",
    "#transform = tv.transforms.Compose([tv.transforms.ToTensor(),\n",
    "#                                tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "datasets = {\n",
    "    'train': tv.datasets.MNIST(os.path.join('mnist', 'train'), \n",
    "                               train=True, \n",
    "                               download=True, \n",
    "                               transform=transform),\n",
    "    'test': tv.datasets.MNIST(os.path.join('mnist', 'test'), \n",
    "                              train=False, \n",
    "                              download=True, \n",
    "                              transform=transform),    \n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    name: torch.utils.data.DataLoader(value, batch_size=100, shuffle=True) for name, value in datasets.items()\n",
    "}    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.5, .5, 0.5])\n",
    "        std = np.array([0.5, 0.5, 0.5])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 233,
       "width": 233
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(dataloaders['train']))\n",
    "imshow(image[0])\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model for training Variational Auto Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, code_size, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.code_size = code_size # size of latent vector z.\n",
    "        self.input_size = input_size\n",
    "        self.encoder_fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc_mu = torch.nn.Linear(hidden_size, code_size)\n",
    "        self.fc_logvar = torch.nn.Linear(hidden_size, code_size)\n",
    "        \n",
    "        self.decoder_fc1 = torch.nn.Linear(code_size, hidden_size)\n",
    "        self.decoder_fc2 = torch.nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def fit(self, dataloader, device, num_epochs, learn_rate):\n",
    "        \"\"\"\n",
    "        \"\"\"    \n",
    "        optimizer  = torch.optim.Adam(self.parameters(), lr=learn_rate)\n",
    "\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        for epoch in range(num_epochs):\n",
    "            acc = []\n",
    "            for x, _ in iter(dataloader('train')):\n",
    "                self.train()\n",
    "                x = x.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                x_hat, mu, logvar = self.forward(x)\n",
    "                loss = Model.vae_loss(x_hat, x, mu, logvar)\n",
    "                acc.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss.append(np.mean(acc))\n",
    "            loss = self.validate(dataloader('test'), device)\n",
    "            test_loss.append(loss)\n",
    "            print(\"Epoch={} Loss={}\".format(epoch, loss))\n",
    "        return train_loss, test_loss\n",
    "    \n",
    "    def validate(self, dataloader, device):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            acc = []\n",
    "            for x, _ in iter(dataloader):\n",
    "                x = x.to(device)\n",
    "                x_hat, mu, logvar = self.forward(x)\n",
    "                loss = Model.vae_loss(x_hat, x, mu, logvar)\n",
    "                acc.append(loss.item())\n",
    "            return np.mean(acc)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\" VAE encoder step.\n",
    "            Return vectors mu and log_variance for\n",
    "            distribution Q(z|x) that approximates P(z|x).\n",
    "        \"\"\"\n",
    "        h = F.relu(self.encoder_fc1(x))\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\" VAE decoder step.\n",
    "            Return reconstructed vector x_hat.\n",
    "        \"\"\"\n",
    "        h = F.relu(self.decoder_fc1(z))\n",
    "        return torch.sigmoid(self.decoder_fc2(h))\n",
    "\n",
    "    def reparam(self, mu, logvar):\n",
    "        \"\"\" Return vector representing latent state - z.\n",
    "            Sample z - latent vector - from \n",
    "            distribution Q(z|x).\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            sigma = torch.exp(0.5*logvar)\n",
    "            e = torch.randn_like(sigma)\n",
    "            return e.mul(sigma).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward step VAE encoder / decoder.\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x.view(-1, self.input_size))\n",
    "        z = self.reparam(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def infer(self, x):\n",
    "        \"\"\" Return z - latent state sampled from Q(z|x)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.encode(x.view(-1, self.input_size))\n",
    "            return self.reparam(mu, logvar)\n",
    "        \n",
    "    @staticmethod\n",
    "    def vae_loss(x_hat, x, mu, log_variance):\n",
    "        \"\"\" Return value of lower bound of data log likelihood:\n",
    "                sum of reconstruction error and divergence between\n",
    "                Q(z|x) and P(z|x).\n",
    "        \"\"\"\n",
    "        batch_size, x_size = x_hat.shape\n",
    "        bce = F.binary_cross_entropy(x_hat, x.view(-1, x_size))\n",
    "\n",
    "        divergence = -0.5 * torch.sum(1. + log_variance - mu.pow(2) - log_variance.exp())\n",
    "        divergence /= (batch_size * x_size)\n",
    "        return bce + divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_device(use_gpu):\n",
    "    \"\"\" Return device.\n",
    "    \"\"\"\n",
    "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \\\n",
    "           if use_gpu \\\n",
    "           else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch=0 Loss=0.21452903538942336\n",
      "Epoch=1 Loss=0.20618663236498833\n"
     ]
    }
   ],
   "source": [
    "N = 28*28\n",
    "model = Model(N, 2, 500)\n",
    "device = make_device(True)\n",
    "model.to(device)\n",
    "model.fit(lambda name: dataloaders[name], device, 3, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in iter(dataloaders['train']):\n",
    "        x_hat, _, _ = model.forward(x)\n",
    "        im = x_hat.detach().numpy()\n",
    "        im.resize(x_hat.shape[0], 1, 28, 28)\n",
    "        imshow(torch.Tensor(im[0]))\n",
    "                  \n",
    "          # for the first 128 batch of the epoch, show the first 8 input digits\n",
    "          # with right below them the reconstructed output digits\n",
    "        n = min(x.size(0), 8)\n",
    "        comparison = torch.cat([x[:n],\n",
    "                                  x_hat.view(x.shape[0], 1, 28, 28)[:n]])\n",
    "        tv.utils.save_image(comparison.data.cpu(),\n",
    "                     'reconstruction_' + str(1) + '.png', nrow=n)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hidden_state(model, dataloader):\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    label = []\n",
    "    for x, y in iter(dataloader):\n",
    "        z = model.infer(x)\n",
    "        x1.extend(z[:, 0].numpy())\n",
    "        x2.extend(z[:, 1].numpy())\n",
    "        label.extend(y)\n",
    "    plt.scatter(x1, x2, c=label, cmap='tab10')\n",
    "    plt.colorbar()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hidden_state(model, dataloaders['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
